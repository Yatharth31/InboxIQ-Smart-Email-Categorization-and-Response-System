{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarities_btw_emails.ipynb file to find the similarity between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from email.parser import Parser\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir1 = r\"C:\\Users\\rohit\\Downloads\\enron_mail_20150507.tar\\enron_mail_20150507\\maildir\\lay-k\\all_documents\\33_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir2 = r\"C:\\Users\\rohit\\Downloads\\enron_mail_20150507.tar\\enron_mail_20150507\\maildir\\lay-k\\all_documents\\34_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir1,\"r\") as f:\n",
    "    data1 = f.read()\n",
    "email1 = Parser().parsestr(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir2,\"r\") as f:\n",
    "    data2 = f.read()\n",
    "email2 = Parser().parsestr(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking two email file for exicuting the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = email1.get_payload().replace(\"\\n\",\"\")\n",
    "text2 = email2.get_payload().replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Lay,  I have received the proposal from Gulstream and will drop it off today.  As you probably already know, the flight from Beijing was a record first for the GV into Houston.I have met and discussed the possibility of the GV replacing a 900B or a total fleet transition at length with John Stabler, Ron Fain and Keith Jones.  I have prepared a spread sheet showing the affect on budget as well as our thoughts and recommendations.  Although the cabin size of the Global Express is larger, Gulfstream is a good two years ahead of the Global in deliveries and fleet size.  They have overcome the mechanical problems that were experienced earlier in the program,  problems that  the Global Express may still be facing.  Worldwide support and operational experience have the Gulfstream far ahead of the Global Express.  Bombardier is still looking forward to demonstrating their aircraft to you on your India trip in December.All of us at the department stand ready to do whatever you feel necessary to support Enron into the new millennium.       '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to tokenize the the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(raw):\n",
    "    wordlist = nltk.word_tokenize(raw)\n",
    "    text = [w.lower() for w in wordlist if w not in stopwords_en]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = preprocessing(text1)\n",
    "text2 = preprocessing(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating frequency dictionary for the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set(text1).union(set(text2))\n",
    "\n",
    "freqd_text1 = FreqDist(text1)\n",
    "text1_count_dict = dict.fromkeys(word_set,0)\n",
    "for word in text1:\n",
    "    text1_count_dict[word] = freqd_text1[word]\n",
    "\n",
    "freqd_text2 = FreqDist(text2)\n",
    "text2_count_dict = dict.fromkeys(word_set,0)\n",
    "for word in text2:\n",
    "    text2_count_dict[word] = freqd_text2[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 6, 'president': 3, 'congress': 3, 'operations': 2, 'authority': 2, 'extend': 2, 'exemption': 2, 'congressional': 2, 'government': 2, 'clinton': 1, ...})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqd_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 7, ',': 5, 'global': 4, 'express': 3, 'i': 2, 'gv': 2, 'fleet': 2, 'size': 2, 'gulfstream': 2, 'ahead': 2, ...})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqd_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqd_text1 = FreqDist(text1)\n",
    "text1_length = len(text1)\n",
    "text1_tf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text1:\n",
    "    text1_tf_dict[word] = freqd_text1[word]/text1_length\n",
    "\n",
    "freqd_text2 = FreqDist(text2)\n",
    "text2_length = len(text2)\n",
    "text2_tf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text2:\n",
    "    text2_tf_dict[word] = freqd_text2[word]/text2_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "text12_idf_dict = dict.fromkeys(word_set,0)\n",
    "text12_length = 2\n",
    "\n",
    "for word in text12_idf_dict.keys():\n",
    "    if word in text1:\n",
    "        text12_idf_dict[word] += 1\n",
    "    if word in text2:\n",
    "        text12_idf_dict[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, val in text12_idf_dict.items():\n",
    "    text12_idf_dict[word] = 1 + math.log(text12_length/(float(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_tfidf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text1:\n",
    "    text1_tfidf_dict[word] = (text1_tf_dict[word]) * (text12_idf_dict[word])\n",
    "\n",
    "text2_tfidf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text2:\n",
    "    text2_tfidf_dict[word] = (text2_tf_dict[word]) * (text12_idf_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggeddocs = list()\n",
    "doc1 = TaggedDocument(words=text1,tags= [\"Mail_1\"])\n",
    "taggeddocs.append(doc1)\n",
    "doc2 = TaggedDocument(words=text2,tags= [\"Mail_2\"])\n",
    "taggeddocs.append(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['mr.', 'lay', ',', 'i', 'received', 'proposal', 'gulstream', 'drop', 'today', '.', 'as', 'probably', 'already', 'know', ',', 'flight', 'beijing', 'record', 'first', 'gv', 'houston.i', 'met', 'discussed', 'possibility', 'gv', 'replacing', '900b', 'total', 'fleet', 'transition', 'length', 'john', 'stabler', ',', 'ron', 'fain', 'keith', 'jones', '.', 'i', 'prepared', 'spread', 'sheet', 'showing', 'affect', 'budget', 'well', 'thoughts', 'recommendations', '.', 'although', 'cabin', 'size', 'global', 'express', 'larger', ',', 'gulfstream', 'good', 'two', 'years', 'ahead', 'global', 'deliveries', 'fleet', 'size', '.', 'they', 'overcome', 'mechanical', 'problems', 'experienced', 'earlier', 'program', ',', 'problems', 'global', 'express', 'may', 'still', 'facing', '.', 'worldwide', 'support', 'operational', 'experience', 'gulfstream', 'far', 'ahead', 'global', 'express', '.', 'bombardier', 'still', 'looking', 'forward', 'demonstrating', 'aircraft', 'india', 'trip', 'december.all', 'us', 'department', 'stand', 'ready', 'whatever', 'feel', 'necessary', 'support', 'enron', 'new', 'millennium', '.'], ['Mail_1'])\n"
     ]
    }
   ],
   "source": [
    "print(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Gensim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Doc2Vec(taggeddocs,dm=0,alpha=0.025,vector_size=20,min_alpha=0.025,min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = sum([len(sentence) for sentence in taggeddocs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(80):\n",
    "    if epoch % 20 ==0:\n",
    "        print(\"Now training epoch %s\" %epoch)\n",
    "    model.train(taggeddocs,total_examples = token_count, epochs = 80)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Index: 11.36 %\n"
     ]
    }
   ],
   "source": [
    "v1 = list(text1_tfidf_dict.values())\n",
    "v2 = list(text2_tfidf_dict.values())\n",
    "\n",
    "similarity = 1 - nltk.cluster.cosine_distance(v1,v2)\n",
    "print(\"Similarity Index: {:4.2f} %\".format(similarity*100))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
