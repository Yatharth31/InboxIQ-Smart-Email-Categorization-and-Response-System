{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing_Similarity.ipynb program is for applying the Similarity_btw_emails program for recived email with all the other email "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from email.parser import Parser\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'enron_mail_20150507.tar\\\\enron_mail_20150507\\\\maildir\\\\lay-k\\\\all_documents\\\\33_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-96e332ea34de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrootdir1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"enron_mail_20150507.tar\\enron_mail_20150507\\maildir\\lay-k\\all_documents\\33_\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrootdir1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0memail1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsestr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'enron_mail_20150507.tar\\\\enron_mail_20150507\\\\maildir\\\\lay-k\\\\all_documents\\\\33_'"
     ]
    }
   ],
   "source": [
    "rootdir1 = r\"enron_mail_20150507.tar\\enron_mail_20150507\\maildir\\lay-k\\all_documents\\33_\"\n",
    "with open(rootdir1,\"r\") as f:\n",
    "    data1 = f.read()\n",
    "email1 = Parser().parsestr(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"enron_mail_20150507.tar\\\\enron_mail_20150507\\\\maildir\\\\lay-k\\\\elizabeth\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating class for the function the checking similarity btw documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Similarity:\n",
    "    stopwords_en = stopwords.words(\"english\")\n",
    "    def __init__(self,rec_mail,email_dir):        \n",
    "        self.rec_mail = rec_mail\n",
    "        self.similirity_list = list()\n",
    "        self.rootdir = email_dir\n",
    "        self.top5 = list()\n",
    "        self.email_body = \"\"\n",
    "    \n",
    "    def email_body_single(self,inputfile):\n",
    "        with open(inputfile,\"r\") as f:\n",
    "            data=f.read()\n",
    "\n",
    "        email = Parser().parsestr(data)\n",
    "\n",
    "        self.email_body= self.email_body + (email.get_payload().replace(\"\\n\",\"\"))\n",
    "\n",
    "    def email_body_sum(self):\n",
    "        for directory, subdirectory,filenames in os.walk(rootdir):\n",
    "            for filename in  filenames:\n",
    "                try:\n",
    "                    self.email_body_single(os.path.join(directory,filename))\n",
    "                except:\n",
    "                    print(os.path.join(directory,filename))\n",
    "\n",
    "    def preprocessing(self,raw):\n",
    "        wordlist = nltk.word_tokenize(raw)\n",
    "        text = [w.lower() for w in wordlist if w not in self.stopwords_en]\n",
    "        return text\n",
    "\n",
    "    def email_analyse(self,inputfile):\n",
    "\n",
    "        with open(inputfile,\"r\") as f:\n",
    "            data2=f.read()\n",
    "    \n",
    "        email2 = Parser().parsestr(data2)\n",
    "\n",
    "\n",
    "        text1 = self.preprocessing(self.rec_mail.replace(\"\\n\",\"\"))\n",
    "        text2 = self.preprocessing(email2.get_payload().replace(\"\\n\",\"\"))\n",
    "\n",
    "        word_set = set(text1).union(set(text2))\n",
    "        word_set_idf = set(text1).union(set(text2))#set(self.email_body)\n",
    "\n",
    "        freqd_text1 = FreqDist(text1)\n",
    "        text1_count_dict = dict.fromkeys(word_set,0)\n",
    "        for word in text1:\n",
    "            text1_count_dict[word] = freqd_text1[word]\n",
    "\n",
    "        freqd_text2 = FreqDist(text2)\n",
    "        text2_count_dict = dict.fromkeys(word_set,0)\n",
    "        for word in text2:\n",
    "            text2_count_dict[word] = freqd_text2[word]\n",
    "        \n",
    "        freqd_text1 = FreqDist(text1)\n",
    "        text1_length = len(text1)\n",
    "        text1_tf_dict = dict.fromkeys(word_set,0)\n",
    "        for word in text1:\n",
    "            text1_tf_dict[word] = freqd_text1[word]/text1_length\n",
    "\n",
    "        freqd_text2 = FreqDist(text2)\n",
    "        text2_length = len(text2)\n",
    "        text2_tf_dict = dict.fromkeys(word_set,0)\n",
    "        for word in text2:\n",
    "            text2_tf_dict[word] = freqd_text2[word]/text2_length\n",
    "        \n",
    "        text12_idf_dict = dict.fromkeys(word_set_idf,0)\n",
    "        text12_length = 2\n",
    "\n",
    "        for word in text12_idf_dict.keys():\n",
    "            if word in text1:\n",
    "                text12_idf_dict[word] += 1\n",
    "            if word in text2:\n",
    "                text12_idf_dict[word] += 1\n",
    "        \n",
    "        for word, val in text12_idf_dict.items():\n",
    "            if val!=0:\n",
    "                text12_idf_dict[word] = 1 + math.log(text12_length/(float(val)))\n",
    "        \n",
    "        text1_tfidf_dict = dict.fromkeys(word_set,0)\n",
    "        for word in text1:\n",
    "            text1_tfidf_dict[word] = (text1_tf_dict[word]) * (text12_idf_dict[word])\n",
    "\n",
    "        text2_tfidf_dict = dict.fromkeys(word_set,0)\n",
    "        for word in text2:\n",
    "            text2_tfidf_dict[word] = (text2_tf_dict[word]) * (text12_idf_dict[word])\n",
    "        \n",
    "        taggeddocs = list()\n",
    "        doc1 = TaggedDocument(words=text1,tags= [\"Mail_1\"])\n",
    "        taggeddocs.append(doc1)\n",
    "        doc2 = TaggedDocument(words=text2,tags= [\"Mail_2\"])\n",
    "        taggeddocs.append(doc2)\n",
    "\n",
    "        model = gensim.models.Doc2Vec(taggeddocs,dm=0,alpha=0.025,vector_size=20,min_alpha=0.025,min_count=0)\n",
    "        token_count = sum([len(sentence) for sentence in taggeddocs])\n",
    "        for epoch in range(80):\n",
    "            if epoch % 20 ==0:\n",
    "                print(\"Now training epoch %s\" %epoch)\n",
    "            model.train(taggeddocs,total_examples = token_count, epochs = 80)\n",
    "            model.alpha -= 0.002\n",
    "            model.min_alpha = model.alpha\n",
    "        \n",
    "        v1 = list(text1_tfidf_dict.values())\n",
    "        v2 = list(text2_tfidf_dict.values())\n",
    "\n",
    "        similarity = 1 - nltk.cluster.cosine_distance(v1,v2)\n",
    "        self.similirity_list.append(similarity)\n",
    "        print(\"Similarity Index: {:4.2f} %\".format(similarity*100))\n",
    "\n",
    "\n",
    "\n",
    "    def check_similarity(self):\n",
    "        for directory, subdirectory,filenames in os.walk(self.rootdir):\n",
    "            for filename in  filenames:\n",
    "                # try:\n",
    "                self.email_analyse(os.path.join(directory,filename))\n",
    "                # except:\n",
    "                #     print(\"Error: \",os.path.join(directory,filename))\n",
    "        \n",
    "    def display_similarities(self):\n",
    "        for i in range (5):\n",
    "            self.top5.append(max(self.similirity_list))\n",
    "            self.similirity_list.remove(max(self.similirity_list))\n",
    "        print(self.similirity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Obj = Similarity(email1.get_payload(),rootdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Obj.email_body_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 17.26 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 13.69 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 16.33 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 16.98 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 15.94 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 5.78 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 23.85 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 9.13 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 6.31 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 21.87 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 16.30 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 28.92 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 24.23 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 18.47 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 0.71 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 100.00 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 11.36 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 14.54 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 14.77 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 3.55 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 7.64 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 3.41 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 11.09 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 21.01 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 6.82 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 12.57 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 13.93 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 15.42 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 16.63 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 14.54 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 0.00 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 2.19 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 14.38 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 26.22 %\n",
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n",
      "Similarity Index: 24.18 %\n"
     ]
    }
   ],
   "source": [
    "Obj.check_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17255341069937424, 0.1368944884372253, 0.1632895635946925, 0.16983252667167914, 0.15938086247429695, 0.05776345370057989, 0.23849064124259867, 0.09131335472552704, 0.06314764008507079, 0.21869178760314223, 0.16298013416373291, 0.18474541061907335, 0.00714036020633646, 0.1136480350397262, 0.14540446962568, 0.14768816757350112, 0.035521337988735024, 0.076408851366907, 0.034095580742622644, 0.11094838890413594, 0.21005735033097683, 0.06822070022727211, 0.12569659144486045, 0.1392510950843624, 0.15415709172554248, 0.1662684963180796, 0.14540057439964882, 0.0, 0.021858557192827255, 0.1438185621390231]\n"
     ]
    }
   ],
   "source": [
    "Obj.display_similarities()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
